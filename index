import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

# —————————— Model Components ——————————

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.bn1   = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.bn2   = nn.BatchNorm2d(channels)
        self.act   = nn.ReLU(inplace=True)
    def forward(self, x):
        r = self.act(self.bn1(self.conv1(x)))
        r = self.bn2(self.conv2(r))
        return self.act(x + r)

class GraphReasoning(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc_q   = nn.Linear(dim, dim)
        self.fc_k   = nn.Linear(dim, dim)
        self.fc_v   = nn.Linear(dim, dim)
        self.update = nn.Linear(dim, dim)
    def forward(self, feats):
        q = self.fc_q(feats)
        k = self.fc_k(feats)
        v = self.fc_v(feats)
        A = F.softmax(torch.bmm(q, k.transpose(1,2)) / feats.size(-1)**0.5, dim=-1)
        agg = torch.bmm(A, v)
        return self.update(agg)

class ExternalMemory(nn.Module):
    def __init__(self, mem_size=64, dim=256):
        super().__init__()
        self.register_buffer('mem', torch.randn(mem_size, dim))
        self.read_w      = nn.Linear(dim, mem_size)
        self.write_w     = nn.Linear(dim, mem_size)
        self.controller  = nn.GRUCell(dim, dim)
    def forward(self, x):
        w_r = F.softmax(self.read_w(x), dim=-1)
        w_w = F.softmax(self.write_w(x), dim=-1)
        read_vec = w_r @ self.mem
        delta = (w_w.unsqueeze(-1) * x.unsqueeze(1)).mean(0)
        with torch.no_grad():
            self.mem += delta
        return self.controller(x, read_vec)

class DynamicFusionNet(nn.Module):
    def __init__(self, img_ch=3, prop_dim=32, hidden=256, n_heads=8, n_layers=6):
        super().__init__()
        self.stem = nn.Sequential(
            nn.Conv2d(img_ch, hidden//2, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(hidden//2), nn.ReLU(inplace=True),
            ResidualBlock(hidden//2), ResidualBlock(hidden//2)
        )
        self.proj_token = nn.Linear(hidden//2, hidden)
        self.prop_enc   = nn.Sequential(
            nn.Linear(prop_dim, hidden), nn.ReLU(inplace=True),
            nn.Linear(hidden, hidden),     nn.ReLU(inplace=True)
        )
        layer = nn.TransformerEncoderLayer(d_model=hidden, nhead=n_heads, dim_feedforward=hidden*4)
        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)
        self.graph       = GraphReasoning(hidden)
        self.memory      = ExternalMemory(mem_size=32, dim=hidden)
        self.head        = nn.Sequential(
            nn.LayerNorm(hidden),
            nn.Linear(hidden, hidden//2), nn.ReLU(inplace=True),
            nn.Linear(hidden//2, hidden)
        )
    def forward(self, x_img, x_prop):
        v   = self.stem(x_img).flatten(2).transpose(1,2)
        v   = self.proj_token(v)
        p   = self.prop_enc(x_prop).unsqueeze(1)
        seq = torch.cat([v, p], dim=1)
        t   = self.transformer(seq.transpose(0,1)).transpose(0,1)
        g   = self.graph(t)
        agg = g.mean(dim=1)
        m   = self.memory(agg)
        return self.head(m)

# —————————— Dummy Dataset ——————————

class RobotDataset(Dataset):
    def __init__(self, N):
        self.N = N
    def __len__(self):
        return self.N
    def __getitem__(self, i):
        img   = torch.randn(3,224,224)
        prop  = torch.randn(32)
        label = torch.randint(0,10,(1,)).item()
        return img, prop, label

# —————————— Training & Logging ——————————

def train_and_log(epochs=10, batch_size=16):
    device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model     = DynamicFusionNet().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    loader    = DataLoader(RobotDataset(1000), batch_size=batch_size, shuffle=True)
    writer    = SummaryWriter('runs/brain_evolution')

    sample_img, sample_prop, _ = next(iter(loader))
    writer.add_graph(model, (sample_img.to(device), sample_prop.to(device)))

    step = 0
    for epoch in range(1, epochs+1):
        model.train()
        loss_sum = 0
        for imgs, props, labels in loader:
            imgs, props, labels = imgs.to(device), props.to(device), labels.to(device)
            out    = model(imgs, props)                       
            logits = nn.Linear(out.size(-1), 10).to(device)(out)
            loss   = criterion(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            writer.add_scalar('Loss/train', loss.item(), step)
            if step % 100 == 0:
                for name, param in model.named_parameters():
                    writer.add_histogram(name, param, step)
            step += 1
            loss_sum += loss.item()

        avg = loss_sum / len(loader)
        print(f"Epoch {epoch}/{epochs}  avg_loss={avg:.4f}")
        writer.add_scalar('Loss/epoch', avg, epoch)

    writer.close()

if __name__ == "__main__":
    train_and_log()
